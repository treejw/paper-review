# Word Embedding

### âœ” Word Embedding
- ë‹¨ì–´ë¥¼ ë²¡í„°(**Dense Representation**)ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•
- Word Embedding ì¢…ë¥˜: LSA, Word2Vec, FastText, GloVe ë“±


<br>

### â—¼ **Sparse Representation** vs **Dense Representation**
- **Sparse Representation (í¬ì†Œ í‘œí˜„)**
   - **one-hot ë²¡í„°** - í‘œí˜„í•˜ë ¤ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ ê°’ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ ë²¡í„° `ê°œ = [1 0 0 0 ... ]`

   - **ë‹¨ì **
      - ë‹¨ì–´ì˜ ì§‘í•©ì´ ì»¤ì§ˆìˆ˜ë¡ ì°¨ì›ì´ ì»¤ì§„ë‹¤.
      - ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì§€ ëª»í•˜ëŠ” í‘œí˜„ ë°©ë²•ì´ë‹¤.

- **Dense Representation (ë°€ì§‘ í‘œí˜„)**
   - ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ìœ¼ë¡œ ë‹¨ì–´ì˜ ë²¡í„° ê¸¸ì´ê°€ ê³ ì •ëœë‹¤.
   - ë²¡í„°ì˜ ê°’ì´ 0ê³¼ 1ë¡œë§Œ í‘œí˜„ë˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ì•„ë˜ì™€ ê°™ì´ **ì‹¤ìˆ˜**ë¡œ í‘œí˜„.
   -  ë²¡í„° ì°¨ì›ì´ 128ì¼ ë•Œ, `ê°œ = [0.2 1.8 1.1 -2.1 1.1 ... ì¤‘ëµ ... ]`
 
   - **íŠ¹ì§• ë° ì¥ì **
      - Sparse Representationì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì°¨ì›ìœ¼ë¡œ ë‹¨ì–´ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
      - Dense Representationë¥¼ ì›Œë“œ ì„ë² ë”© ê³¼ì •ì„ í†µí•´ ë‚˜ì˜¨ ê²°ê³¼ë¼ê³  í•˜ì—¬ ì„ë² ë”© ë²¡í„°(embedding vector)ë¼ê³ ë„ í•œë‹¤.


<br><br>


### âœ” Word2Vec
- **Distributed Representation (ë¶„ì‚° í‘œí˜„)** ë°©ë²•ì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
- CBOWì™€ skip-Gram 2ê°€ì§€ ë°©ì‹ì´ ì¡´ì¬
   - **[CBOW (Continuous Bag of Words)](https://wikidocs.net/22660#3-cbowcontinuous-bag-of-words)**
      - ì£¼ë³€ì— ìˆëŠ” ë‹¨ì–´ë“¤ë¡œ, **ì¤‘ê°„ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡**í•˜ëŠ” ë°©ë²•
   - **[Skip-Gram](https://wikidocs.net/22660#4-skip-gram)**
      - ì¤‘ê°„ì— ìˆëŠ” ë‹¨ì–´ë¡œ, **ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡**í•˜ëŠ” ë°©ë²•. (CBOWì™€ ë°˜ëŒ€)

![](https://miro.medium.com/max/875/1*i-aWU_fjKblzRG4OTgmCkA.png)

<br><br>

### âœ” GloVe(Global Vectors for Word Representation)
- **ì¹´ìš´íŠ¸ ê¸°ë°˜**ê³¼ **ì˜ˆì¸¡ ê¸°ë°˜**ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ 
   - ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSAì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ì˜ ë°©ë²•. 
- Word2Vec ë§Œí¼ ì„±ëŠ¥ì´ ì¢‹ìŒ.
- **GloVeì˜ ì•„ì´ë””ì–´**
   - **ì„ë² ë”© ëœ ì¤‘ì‹¬ ë‹¨ì–´ì™€ ì£¼ë³€ ë‹¨ì–´ ë²¡í„°ì˜ ë‚´ì **ì´ **ì „ì²´ ì½”í¼ìŠ¤ì—ì„œì˜ ë™ì‹œ ë“±ì¥ í™•ë¥ **ì´ ë˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ


![](https://miro.medium.com/max/875/1*2HuruOHvhP7_gnW2DKB2FQ.png)



<br><br><br>

## ELMo(Embeddings from Language Model) 
### âœ” Motivation
 - Word2Vecê³¼ GloVeì˜ í•œê³„
   - ë‹¤ì–‘í•œ ì˜ë¯¸ë¥¼ ì§€ë‹Œ í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ì„ë² ë”© ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤.
   - ì˜ˆ) Bank Account(ì€í–‰ ê³„ì¢Œ) | River Bank(ê°•ë‘‘)
- Idea: ë‹¨ì–´ë¥¼ ì„ë² ë”©í•˜ê¸° ì „, ì „ì²´ ë¬¸ì¥ì„ ê³ ë ¤í•´ì„œ ì„ë² ë”©ì„ í•˜ê² ë‹¤.

   â–¶ **ë¬¸ë§¥ì„ ë°˜ì˜í•œ ë‹¨ì–´ ì„ë² ë”© (Contextualized Word Embedding)**

<br>

### âœ” Proposed Method
#### ğŸ”¸ Bidirectional Language Model (biLM)

<img src="https://wikidocs.net/images/page/33930/forwardbackwordlm2.PNG">

- ELMoëŠ” ì–¸ì–´ ëª¨ë¸ë¡œì¨ **ì–‘ë°©í–¥ LSTM(biLM)** ì„ ì´ìš©.
   - forward LM
      -  tkâˆ’1ê¹Œì§€ì˜ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ê°€ì§€ê³  tkì˜ í™•ë¥  ê³„ì‚°
      
      <img height="65;" src="https://user-images.githubusercontent.com/42428487/100283235-721a2e00-2fb0-11eb-985c-929663d78157.png">

   - backward LM
      
      <img height="60;" src="https://user-images.githubusercontent.com/42428487/100283653-40ee2d80-2fb1-11eb-8a99-132ed4941471.png">

   - biLMì€ forward LMê³¼ backward LM ê°ê°ì— ëŒ€í•œ log likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.

      <img height="110;" src="https://user-images.githubusercontent.com/42428487/100285318-0934b500-2fb4-11eb-98f8-13470ad99553.png">


- biLMì˜ ì…ë ¥ìœ¼ë¡œ `char CNN`ì´ë¼ëŠ” ì„ë² ë”©ì„ ì‚¬ìš©.

<img src="https://wikidocs.net/images/page/33930/playwordvector.PNG">
<img height="70;" src="https://user-images.githubusercontent.com/42428487/100285794-da6b0e80-2fb4-11eb-8e39-e385bc923abc.png">

- `R` : representation ì§‘í•©
- `x` : biLMì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ëœ ë‹¨ì–´ì˜ representation
- `h` : LSTMì˜ ìˆœë°©í–¥/ì—­ë°©í–¥ì˜ representation
- `j` : ëª‡ë²ˆì§¸ layer 
- `k` : ëª‡ë²ˆì§¸ ë‹¨ì–´

<br>

#### ğŸ”¸  ELMo ì„ë² ë”© ì ˆì°¨
1) ê° ì¸µì˜ ì¶œë ¥ê°’ì„ ì—°ê²°(concatenate)
<img src="https://wikidocs.net/images/page/33930/concatenate.PNG">

2) ê° ì¸µì˜ ì¶œë ¥ê°’ ë³„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œë‹¤.
ì´ ê°€ì¤‘ì¹˜ë¥¼ ì—¬ê¸°ì„œëŠ” s<sub>1</sub>, s<sub>2</sub>, s<sub>3</sub>ë¼ê³  ê°€ì •.
<img src="https://wikidocs.net/images/page/33930/concatenate.PNG">

3) ê° ì¸µì˜ ì¶œë ¥ê°’ì„ ëª¨ë‘ ë”í•œë‹¤.
<img src="https://wikidocs.net/images/page/33930/weightedsum.PNG">

4) ë²¡í„°ì˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ìŠ¤ì¹¼ë¼ ë§¤ê°œë³€ìˆ˜(Î³)ë¥¼ ê³±í•œë‹¤.
<img src="https://wikidocs.net/images/page/33930/scalarparameter.PNG">

<img height="60;" src="https://user-images.githubusercontent.com/42428487/100285850-f4a4ec80-2fb4-11eb-92c8-fc5e0e14710a.png">

<br>

#### ğŸ”¸ íŠ¹ì • taskì—ì„œì˜ ELMo ì„ë² ë”© 
<img src="https://wikidocs.net/images/page/33930/elmorepresentation.PNG">


<br><br>



---

### ì°¸ê³ 
- Word Embedding ì •ì˜ 
   - [wikidocs](https://wikidocs.net/33520) | 
   [ckdgus1433 Bolg](https://blog.naver.com/PostView.nhn?blogId=ckdgus1433&logNo=222030454167&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)
- Word2Vec
   - paper: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) 
   - [wikidocs](https://wikidocs.net/22660) | 
   [towards data science](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4)
- GloVe
   - paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) 
   - [wikidocs](https://wikidocs.net/22885)
- ELMo
   - paper: [Deep contextualized word representaions (2018)](https://aclweb.org/anthology/N18-1202)
   - [wikidocs](https://wikidocs.net/33930) | 
   [Baek Kyun Shin Blog](https://bkshin.tistory.com/entry/NLP-12-%EA%B8%80%EB%A1%9C%EB%B8%8CGloVe) | 
   [Dos tacos Blog](https://dos-tacos.github.io/paper%20review/deep-contextualized-word-representations/) | 
   [Youtube](https://www.youtube.com/watch?v=6K3joYQ0DYE)
   
   
